# AI Support Ticket Classifier & Analytics Dashboard

This project provides an automated pipeline to ingest, classify, and visualize customer support tickets using the Google Gemini API. The tool classifies each ticket by **category** and **priority**, and provides a **certainty score** and **reasoning** for each classification.

## Architecture

The tool is divided into four distinct stages:

1. **Intake (`csv_handler.py`):** Loads CSV files, uses LLM reasoning to identify if a header row exists, and prompts the user to confirm which column contains the ticket descriptions.
2. **Classification (`classifier.py`):** Forwards ticket contents to Gemini. It uses JSON schemas to enforce specific categories and priority levels defined in `config.py`.
3. **Analysis (`analyst.py`):** Processes the output data to calculate success rates, average certainty scores, and volume distribution.
4. **Visualization:** Two interfaces available:
   - **Web UI (`app.py`):** A Streamlit-based interactive web interface for guided file upload, header/column confirmation, real-time classification progress, and dashboard visualization.
   - **Terminal UI (`main.py` with `dashboard.py`):** A command-line interface that runs the full pipeline and launches a dashboard at completion.

## Key Technical Features and Decisions

* **Model Selection (gemini-flash-latest):** Version-agnostic alias that ensures maximum uptime. This approach allows the system to automatically track the most recent stable release without manual code changes; more importantly, it provides an internal redirection layer that can fall back to a previous stable version if the newest release encounters availability issues or regional outages.
* **Intelligent Intake:** Uses LLM reasoning to distinguish between labels and data (Header Detection) and to find the "Issue description" column among messy or unknown headers.
* **Human-in-the-Loop (HITL):** Minimizes processing errors by requiring user confirmation for AI-detected headers and target columns before initiating the full pipeline.
* **Automated Quality Triage:** Implements a Confidence Thresholding system that automatically flags low-certainty AI classifications for manual human review.
* **JSON Schema Enforcement:** Guarantees AI responses follow a machine-readable format.
* **Resilience via Tenacity:** To handle the inherent instability of cloud APIs, the system implements exponential backoff. This allows the tool to survive "429 Rate Limit" and "503 Server Overloaded" errors by intelligently waiting and retrying rather than crashing the pipeline.
* **Decoupled Analytics Pipeline:** We separate the data aggregation (`analyst.py`) from the visualization layer. This "Process-then-Display" pattern ensures the dashboard loads instantly for the end-user by reading pre-calculated statistics rather than re-processing raw data on every refresh.
* **Data Integrity:** If classification columns already exist, the tool renames original columns to `{name}_original` to prevent data loss.
* **Config-Driven Architecture (SSOT):** The `config.py` file acts as the "Single Source of Truth."
* **Synthetic Data Generation:** Includes `create_tickets.py` to generate test datasets for development.

## Setup and Installation

Clone the repository and install dependencies:

`pip install -r requirements.txt`

Create a `.env` file in the root directory:

`PROJECT_API_KEY=your_gemini_api_key`

## Usage

### Generating Test Data (Optional)
To generate a sample CSV for testing without using real customer data:

`python create_tickets.py`

### Running the Tool

#### Web UI (Recommended for Interactive Use)
Launch the Streamlit web interface with a guided workflow:

**Option 1 - From Terminal:**
`streamlit run app.py`

**Option 2 - Double-click the batch file:**
`launch.bat`

This opens an interactive web dashboard where you can:
1. **Upload** a CSV file via drag-and-drop
2. **Confirm** header detection with a single click
3. **Select** the ticket description column from available options
4. **Monitor** classification progress in real-time with a progress bar
5. **Explore** results through an interactive dashboard with filters, charts, and download options

#### Terminal Interface
To run the end-to-end workflow from the command line:

`python main.py`

When prompted for a file path, you can:
1. **Press Enter:** Automatically loads the default file (configurable in settings) `data/tickets-10.csv`.
2. **Type a path:** Try one of the edge-case files, such as `data/tickets-10-unusual.csv`, to see the intelligent intake in action.

The terminal interface will complete classification and then automatically launch the dashboard.

## Showcase & Test Datasets

The `/data` folder includes curated datasets to demonstrate the system's flexibility:

* **Standard Sets:** `tickets-10.csv` & `tickets-100.csv` (Baseline testing, generated by `create_tickets.py`).
* **Structural Edge Case:** `tickets-10-unusual.csv` (Validates the LLM's ability to locate the description column regardless of its index or surrounding headers).
* **Index & Semantic Mapping:** `tickets-100-kaggle-1.csv` (Tests the system's ability to ignore leading index columns and correctly map the "Body" header as the target column for ticket descriptions).
* **Semantic Stress Test:** `tickets-100-kaggle-2.csv` (Tests classification against keyword-heavy, non-prose ticket messages).
* **Garbage Data Stress Test:** `tickets-10-gibberish.csv` (Showcases the system's reaction to nonsensical or "garbage" data. Demonstrates how the LLM maintains pipeline stability by gracefully triaging non-actionable input into 'Other/Low' categories rather than crashing or hallucinating critical issues.)
* **Headerless Recovery:** `tickets-10-no-header.csv` (Showcases the LLM's ability to classify data even when column labels are missing, by analyzing the content of the first two rows to determine formatting).

### Data Attribution & Licensing
* `tickets-100-kaggle-1.csv` is a subset of the [IT Support Ticket Dataset](https://www.kaggle.com/datasets/parthpatil256/it-support-ticket-data) used under the **MIT License**.
* `tickets-100-kaggle-2.csv` is a subset of the [All Tickets Dataset](https://www.kaggle.com/datasets/andrewkyalo/all-tickets-csv) used under the **Database Contents License (DbCL) v1.0**.
